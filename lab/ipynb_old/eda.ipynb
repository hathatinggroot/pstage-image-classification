{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a58d5b",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analisys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c0b11",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "349e5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install libgl1-mesa-glx\n",
    "# !pip uninstall -y opencv-contrib-python\n",
    "# !pip uninstall -y opencv-python\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install opencv-python\n",
    "# !pip install seaborn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cc198721-be55-403a-8d23-316e100bd954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "69f33642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_img_dir = os.path.join(train_dir, 'images')\n",
    "train_img_sub_dirs = [os.path.join(train_img_dir, sub_dir) for sub_dir in os.listdir(train_img_dir) if os.path.isdir(os.path.join(train_img_dir, sub_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7ceb2",
   "metadata": {},
   "source": [
    "## 1. Input 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b68c7f",
   "metadata": {},
   "source": [
    "### 1-0. X load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12b2eee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/ml/input/data/train/images/003790_male_Asian_46/mask1.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/mask3.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/incorrect_mask.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/mask2.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/mask5.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/mask4.jpg', '/opt/ml/input/data/train/images/003790_male_Asian_46/normal.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_img_paths = [[os.path.join(sub_dir, img) for img in os.listdir(sub_dir) if not img.startswith('.')]  for sub_dir in train_img_sub_dirs]\n",
    "print(train_img_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299eb7c",
   "metadata": {},
   "source": [
    "### 1-1. X의 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0710fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.shape : (512, 384, 3)\n",
      "X.shape : torch.Size([384, 512, 3])\n",
      "X의 채널수 : 3\n",
      "X의 크기 : 384 x 512\n"
     ]
    }
   ],
   "source": [
    "a, b = (np.random.choice(len(train_img_paths)-1, 1)[0], np.random.choice(7, 1)[0])\n",
    "\n",
    "X_ = cv2.imread(train_img_paths[a][b])\n",
    "print(f'X_.shape : {X_.shape}')\n",
    "\n",
    "X = torch.tensor(X_).permute(1, 0, 2)\n",
    "print(f'X.shape : {X.shape}')\n",
    "\n",
    "print(f'X의 채널수 : {X.shape[2]}')\n",
    "print(f'X의 크기 : {X.shape[0]} x {X.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0deffe-d794-4e6e-abbc-8934407988e3",
   "metadata": {},
   "source": [
    "### 1-2. X의 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd41beec-4d0c-46f9-9883-5116f37a5dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    a, b = (np.random.choice(len(train_img_paths)-1, 1)[0], np.random.choice(7, 1)[0])\n",
    "\n",
    "    X_ = cv2.imread(train_img_paths[a][b])\n",
    "\n",
    "\n",
    "    X = torch.tensor(X_).permute(1, 0, 2)\n",
    "    plt.scatter(X.shape[0], X.shape[1])\n",
    "\n",
    "plt.title('Size of Imgages')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb05bb",
   "metadata": {},
   "source": [
    "### 1-3. 분석 대상이 되는 객체의 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1af90dd0-85c8-4fba-ad6e-8660adb234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544771b9-5493-41ec-b016-c3e973a88a00",
   "metadata": {},
   "source": [
    "### 1-4. RGB 채널별 통계 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9df69401-8716-47e3-9eea-5e36b387d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b5e09-6fec-4c33-be6c-cab38cb23cdd",
   "metadata": {},
   "source": [
    "## 2. Output 분석train_img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef86b26-7000-4470-b5da-7c1e31917f16",
   "metadata": {},
   "source": [
    "### 2-1.target이 될 y에 대한 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fe0f1f7e-9495-4972-856a-443c7361db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  gender   race  age                    path\n",
      "0     000001  female  Asian   45  000001_female_Asian_45\n",
      "1     000002  female  Asian   52  000002_female_Asian_52\n",
      "2     000004    male  Asian   54    000004_male_Asian_54\n",
      "3     000005  female  Asian   58  000005_female_Asian_58\n",
      "4     000006  female  Asian   59  000006_female_Asian_59\n",
      "...      ...     ...    ...  ...                     ...\n",
      "2695  006954    male  Asian   19    006954_male_Asian_19\n",
      "2696  006955    male  Asian   19    006955_male_Asian_19\n",
      "2697  006956    male  Asian   19    006956_male_Asian_19\n",
      "2698  006957    male  Asian   20    006957_male_Asian_20\n",
      "2699  006959    male  Asian   19    006959_male_Asian_19\n",
      "\n",
      "[2700 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "train_info = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "print(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f96b911a-c7f6-4f9a-861b-2488225703e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "(0, 30]     1298\n",
      "(30, 60]    1402\n",
      "(60, 90]       0\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ratio_age = train_info['age'].groupby(pd.cut(train_info['age'], np.arange(0, 120, 30))).count()\n",
    "print(ratio_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "aa59c6f6-936c-47f8-811e-70595c371e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 30, 60, 90])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 120, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d028606f-3d13-4618-ad5e-783dc833a7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-68949b9acb9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mratio_age\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-68949b9acb9d>\u001b[0m in \u001b[0;36mage_process\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mage_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# 클래스 요소인 gender 분포\n",
    "ratio_gender = train_info[['gender', 'path']].groupby(['gender']).count().to_dict()['path']\n",
    "\n",
    "def age_process(x: int) -> int:\n",
    "    if x<30:\n",
    "        return 0\n",
    "    elif 30<=x<60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "ratio_age = train_info[['age']].apply(age_process)\n",
    "\n",
    "\n",
    "ratio_age = train_info[['age', 'path']].groupby(['age']).count().to_dict()['path']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "axs[0, 0].pie(ratio_gender.values(), labels=ratio_gender.keys(), autopct='%1.1f%%')\n",
    "axs[0, 1].pie(ratio_age.values(), labels=ratio_age.keys(), autopct='%1.1f%%')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8436d",
   "metadata": {},
   "source": [
    "## 3. 관계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b7da5bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train/train/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-5d5297126511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_per_person\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_info_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_info_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train/train/images'"
     ]
    }
   ],
   "source": [
    "train_dir = '../data/train/train'\n",
    "train_image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "data_per_person = os.listdir(train_image_dir)\n",
    "train_info_path = os.path.join(train_dir, 'train.csv')\n",
    "train_info = pd.read_csv(train_info_path) \n",
    "print(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc139b-67ff-4efe-b1e0-35837f4a6f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3668d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  gender   race  age                    path\n",
      "0  000001  female  Asian   45  000001_female_Asian_45\n",
      "   gender  age\n",
      "0  female   45\n",
      "   gender  age\n",
      "0  female   45\n",
      "\n",
      "female 45\n"
     ]
    }
   ],
   "source": [
    "qr = train_info[train_info.path == '000001_female_Asian_45']\n",
    "print(qr)\n",
    "\n",
    "print(qr[['gender', 'age']])\n",
    "a  = qr[['gender', 'age']]\n",
    "print(a)\n",
    "print()\n",
    "[x, y] = a.values[0]\n",
    "print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6187124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n",
      "2700\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# train.csv의 path 정보와 실제 이미지 정보 일치 여부 확인\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "c_from_dir = Counter(data_per_person)\n",
    "print(len(c_from_dir))\n",
    "\n",
    "c_from_csv = Counter(train_info['path'].unique())\n",
    "print(len(c_from_csv))\n",
    "\n",
    "print(c_from_csv == c_from_dir)\n",
    "\n",
    "print(any(filter(lambda a: a != 1, c_from_csv.values())))\n",
    "\n",
    "train_image_sub_dirs = [os.path.join(train_image_dir, sub_dir) for sub_dir in train_info['path'].unique()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfda3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 이름 : \n",
      "Counter({'incorrect_mask': 2700, 'mask4': 2700, 'mask5': 2700, 'mask1': 2700, 'normal': 2700, 'mask2': 2700, 'mask3': 2700})\n",
      "확장자 : \n",
      "Counter({'jpg': 18035, 'png': 511, 'jpeg': 354})\n"
     ]
    }
   ],
   "source": [
    "# 사람별 사진 파일 이름과 확장자 확인\n",
    "\n",
    "c_pic_names = Counter()\n",
    "c_pic_ext = Counter()\n",
    "train_image_paths = []\n",
    "\n",
    "def updateCount(fileName):\n",
    "    name, ext = tuple(fileName.split('.'))\n",
    "    \n",
    "    c_pic_names.update([name])\n",
    "    c_pic_ext.update([ext])\n",
    "\n",
    "\n",
    "for i in range(len(train_image_sub_dirs)):\n",
    "    sub_dirs = os.listdir(train_image_sub_dirs[i])\n",
    "    \n",
    "    for sub_dir in sub_dirs:\n",
    "        train_image_paths.append(os.path.join(train_image_sub_dirs[i], sub_dir))\n",
    "        updateCount(sub_dir)\n",
    "    \n",
    "print(f'파일 이름 : \\n{c_pic_names}')\n",
    "print(f'확장자 : \\n{c_pic_ext}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9acf9798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({torch.Size([3, 512, 384]): 18900})\n"
     ]
    }
   ],
   "source": [
    "# input (X) 분석\n",
    "t_train_images = []\n",
    "c_dims = Counter()\n",
    "\n",
    "for train_image_path in train_image_paths:\n",
    "    image = Image.open(train_image_path)\n",
    "    t_image = transforms.ToTensor()(image)\n",
    "    t_train_images.append(t_image)\n",
    "    c_dims.update([t_image.shape])\n",
    "    \n",
    "print(c_dims)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "80634647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect_mask\n",
      "<__main__.Classifier object at 0x7fee40978190>\n",
      "torch.Size([3, 512, 384])\n",
      "tensor([[[ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         [ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         [ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         ...,\n",
      "         [ 0.4216,  0.4412,  0.4412,  ...,  0.4608,  0.4608,  0.4608],\n",
      "         [ 0.3627,  0.3627,  0.3627,  ...,  0.4804,  0.4804,  0.4804],\n",
      "         [ 0.3039,  0.3039,  0.3039,  ...,  0.5392,  0.5392,  0.5392]],\n",
      "\n",
      "        [[ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         [ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         [ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         ...,\n",
      "         [-0.5980, -0.5784, -0.5784,  ..., -0.6569, -0.6569, -0.6569],\n",
      "         [-0.6569, -0.6569, -0.6569,  ..., -0.6373, -0.6373, -0.6373],\n",
      "         [-0.7157, -0.7157, -0.7157,  ..., -0.6569, -0.6569, -0.6569]],\n",
      "\n",
      "        [[ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         [ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         [ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         ...,\n",
      "         [-1.3235, -1.3039, -1.3039,  ..., -1.2451, -1.2451, -1.2451],\n",
      "         [-1.3824, -1.3824, -1.3824,  ..., -1.1667, -1.1667, -1.1667],\n",
      "         [-1.4412, -1.4412, -1.4412,  ..., -1.0686, -1.0686, -1.0686]]])\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhgroot/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "\n",
    "train_dataset = TrainDataset(train_info_path, train_image_paths, transform)\n",
    "X, y = next(iter(train_dataset))\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fd7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, gender: str, age: int, classValue: str ):\n",
    "        self.mask = self._toMask(classValue)\n",
    "        self.gender = gender\n",
    "        self.age = self._toAge(age)\n",
    "\n",
    "    def _toMask(self, classValue) -> str:\n",
    "        if classValue == 'incorrect_mask':\n",
    "            return 'Incorrect'\n",
    "        elif classValue == 'normal':\n",
    "            return 'Not Wear'\n",
    "        else :\n",
    "            return 'Wear'\n",
    "    \n",
    "    def _toAge(self, age: int):\n",
    "        if age < 30:\n",
    "            return 0\n",
    "        elif age >= 60:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def get_class(self):\n",
    "        y = 0\n",
    "        if self.mask == 'Incorrect':\n",
    "            y += 6\n",
    "        elif self.mask == 'Not Wear':\n",
    "            y += 12\n",
    "        \n",
    "        if self.gender == 'female':\n",
    "            y += 3\n",
    "        \n",
    "        y += self.age\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, info_path: str, img_paths: str, transform: transforms.Compose):\n",
    "        self.info = pd.read_csv(os.path.join(info_path))\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        self.classes = [i for i in range(18)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.img_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        dirName, fileName = os.path.split(image_path)\n",
    "        _, person = os.path.split(dirName)\n",
    "        classValue, ext = fileName.split('.')\n",
    "        \n",
    "        print(classValue)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self._toY(person, classValue)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _toY(self, person, classValue) -> torch.Tensor:\n",
    "        qr = self.info[self.info.path == person]\n",
    "        gender, age = qr[['gender', 'age']].values[0]\n",
    "        classifier = Classifier(gender=gender, age=age, classValue=classValue)\n",
    "        print(classifier)\n",
    "        return classifier.get_class()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c38145f413ce776b877e82b073413462501d42396f9d94a4b38839b8b8b6419c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
