{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EDA (Exploratory Data Analisys)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_dir = '../data/train/train'\n",
    "train_img_dir = os.path.join(train_dir, 'images')\n",
    "train_img_sub_dirs = [os.path.join(train_img_dir, sub_dir) for sub_dir in os.listdir(train_img_dir)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_img_sub_dirs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Input 분석"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-0. X load"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train_img_paths = [os.listdir(sub_dir) for sub_dir in train_img_sub_dirs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "torch.randint(high=len(train_img_sub_dirs), size=(2,))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 775, 2198])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-1. X의 feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "\n",
    "target_dir = train_img_sub_dirs[0]\n",
    "os.path.\n",
    "img_path = os.path.join(target_dir, os.listdir(train_img_sub_dirs[0],target_dir)[0])\n",
    "X = Image.open(img_path)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '003106_female_Asian_20'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a6a4e549d6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_img_sub_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '003106_female_Asian_20'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Output 분석"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 관계 분석"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "train_dir = '../data/train/train'\n",
    "train_image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "data_per_person = os.listdir(train_image_dir)\n",
    "train_info_path = os.path.join(train_dir, 'train.csv')\n",
    "train_info = pd.read_csv(train_info_path) \n",
    "print(train_info)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          id  gender   race  age                    path\n",
      "0     000001  female  Asian   45  000001_female_Asian_45\n",
      "1     000002  female  Asian   52  000002_female_Asian_52\n",
      "2     000004    male  Asian   54    000004_male_Asian_54\n",
      "3     000005  female  Asian   58  000005_female_Asian_58\n",
      "4     000006  female  Asian   59  000006_female_Asian_59\n",
      "...      ...     ...    ...  ...                     ...\n",
      "2695  006954    male  Asian   19    006954_male_Asian_19\n",
      "2696  006955    male  Asian   19    006955_male_Asian_19\n",
      "2697  006956    male  Asian   19    006956_male_Asian_19\n",
      "2698  006957    male  Asian   20    006957_male_Asian_20\n",
      "2699  006959    male  Asian   19    006959_male_Asian_19\n",
      "\n",
      "[2700 rows x 5 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "qr = train_info[train_info.path == '000001_female_Asian_45']\n",
    "print(qr)\n",
    "\n",
    "print(qr[['gender', 'age']])\n",
    "a  = qr[['gender', 'age']]\n",
    "print(a)\n",
    "print()\n",
    "[x, y] = a.values[0]\n",
    "print(x, y)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       id  gender   race  age                    path\n",
      "0  000001  female  Asian   45  000001_female_Asian_45\n",
      "   gender  age\n",
      "0  female   45\n",
      "   gender  age\n",
      "0  female   45\n",
      "\n",
      "female 45\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# train.csv의 path 정보와 실제 이미지 정보 일치 여부 확인\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "c_from_dir = Counter(data_per_person)\n",
    "print(len(c_from_dir))\n",
    "\n",
    "c_from_csv = Counter(train_info['path'].unique())\n",
    "print(len(c_from_csv))\n",
    "\n",
    "print(c_from_csv == c_from_dir)\n",
    "\n",
    "print(any(filter(lambda a: a != 1, c_from_csv.values())))\n",
    "\n",
    "train_image_sub_dirs = [os.path.join(train_image_dir, sub_dir) for sub_dir in train_info['path'].unique()]\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2700\n",
      "2700\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# 사람별 사진 파일 이름과 확장자 확인\n",
    "\n",
    "c_pic_names = Counter()\n",
    "c_pic_ext = Counter()\n",
    "train_image_paths = []\n",
    "\n",
    "def updateCount(fileName):\n",
    "    name, ext = tuple(fileName.split('.'))\n",
    "    \n",
    "    c_pic_names.update([name])\n",
    "    c_pic_ext.update([ext])\n",
    "\n",
    "\n",
    "for i in range(len(train_image_sub_dirs)):\n",
    "    sub_dirs = os.listdir(train_image_sub_dirs[i])\n",
    "    \n",
    "    for sub_dir in sub_dirs:\n",
    "        train_image_paths.append(os.path.join(train_image_sub_dirs[i], sub_dir))\n",
    "        updateCount(sub_dir)\n",
    "    \n",
    "print(f'파일 이름 : \\n{c_pic_names}')\n",
    "print(f'확장자 : \\n{c_pic_ext}')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "파일 이름 : \n",
      "Counter({'incorrect_mask': 2700, 'mask4': 2700, 'mask5': 2700, 'mask1': 2700, 'normal': 2700, 'mask2': 2700, 'mask3': 2700})\n",
      "확장자 : \n",
      "Counter({'jpg': 18035, 'png': 511, 'jpeg': 354})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# input (X) 분석\n",
    "t_train_images = []\n",
    "c_dims = Counter()\n",
    "\n",
    "for train_image_path in train_image_paths:\n",
    "    image = Image.open(train_image_path)\n",
    "    t_image = transforms.ToTensor()(image)\n",
    "    t_train_images.append(t_image)\n",
    "    c_dims.update([t_image.shape])\n",
    "    \n",
    "print(c_dims)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({torch.Size([3, 512, 384]): 18900})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "\n",
    "train_dataset = TrainDataset(train_info_path, train_image_paths, transform)\n",
    "X, y = next(iter(train_dataset))\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "incorrect_mask\n",
      "<__main__.Classifier object at 0x7fee40978190>\n",
      "torch.Size([3, 512, 384])\n",
      "tensor([[[ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         [ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         [ 1.2451,  1.2451,  1.2451,  ...,  1.4412,  1.4412,  1.4412],\n",
      "         ...,\n",
      "         [ 0.4216,  0.4412,  0.4412,  ...,  0.4608,  0.4608,  0.4608],\n",
      "         [ 0.3627,  0.3627,  0.3627,  ...,  0.4804,  0.4804,  0.4804],\n",
      "         [ 0.3039,  0.3039,  0.3039,  ...,  0.5392,  0.5392,  0.5392]],\n",
      "\n",
      "        [[ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         [ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         [ 1.2255,  1.2255,  1.2255,  ...,  1.4216,  1.4216,  1.4216],\n",
      "         ...,\n",
      "         [-0.5980, -0.5784, -0.5784,  ..., -0.6569, -0.6569, -0.6569],\n",
      "         [-0.6569, -0.6569, -0.6569,  ..., -0.6373, -0.6373, -0.6373],\n",
      "         [-0.7157, -0.7157, -0.7157,  ..., -0.6569, -0.6569, -0.6569]],\n",
      "\n",
      "        [[ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         [ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         [ 1.1275,  1.1275,  1.1275,  ...,  1.3235,  1.3235,  1.3235],\n",
      "         ...,\n",
      "         [-1.3235, -1.3039, -1.3039,  ..., -1.2451, -1.2451, -1.2451],\n",
      "         [-1.3824, -1.3824, -1.3824,  ..., -1.1667, -1.1667, -1.1667],\n",
      "         [-1.4412, -1.4412, -1.4412,  ..., -1.0686, -1.0686, -1.0686]]])\n",
      "10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/hhgroot/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Classifier():\n",
    "    def __init__(self, gender: str, age: int, classValue: str ):\n",
    "        self.mask = self._toMask(classValue)\n",
    "        self.gender = gender\n",
    "        self.age = self._toAge(age)\n",
    "\n",
    "    def _toMask(self, classValue) -> str:\n",
    "        if classValue == 'incorrect_mask':\n",
    "            return 'Incorrect'\n",
    "        elif classValue == 'normal':\n",
    "            return 'Not Wear'\n",
    "        else :\n",
    "            return 'Wear'\n",
    "    \n",
    "    def _toAge(self, age: int):\n",
    "        if age < 30:\n",
    "            return 0\n",
    "        elif age >= 60:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def get_class(self):\n",
    "        y = 0\n",
    "        if self.mask == 'Incorrect':\n",
    "            y += 6\n",
    "        elif self.mask == 'Not Wear':\n",
    "            y += 12\n",
    "        \n",
    "        if self.gender == 'female':\n",
    "            y += 3\n",
    "        \n",
    "        y += self.age\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, info_path: str, img_paths: str, transform: transforms.Compose):\n",
    "        self.info = pd.read_csv(os.path.join(info_path))\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        self.classes = [i for i in range(18)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.img_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        dirName, fileName = os.path.split(image_path)\n",
    "        _, person = os.path.split(dirName)\n",
    "        classValue, ext = fileName.split('.')\n",
    "        \n",
    "        print(classValue)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self._toY(person, classValue)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _toY(self, person, classValue) -> torch.Tensor:\n",
    "        qr = self.info[self.info.path == person]\n",
    "        gender, age = qr[['gender', 'age']].values[0]\n",
    "        classifier = Classifier(gender=gender, age=age, classValue=classValue)\n",
    "        print(classifier)\n",
    "        return classifier.get_class()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "c38145f413ce776b877e82b073413462501d42396f9d94a4b38839b8b8b6419c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}